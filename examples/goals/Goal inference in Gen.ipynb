{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inferring the goals of autonomous agents in Gen.jl\n",
    "\n",
    "what is Gen.jl (2 sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "using Gen"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Modeling an autonomous agent\n",
    "\n",
    "Explain ( 2 sentence). Explain that a model is a probabilisttic program, and that @program is used to define a probabilistic program in Gen.jl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "include(\"scene.jl\")\n",
    "include(\"path_planner.jl\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "@program agent_model() begin\n",
    "    \n",
    "    # assumed scene\n",
    "    scene = Scene(0, 100, 0, 100) # the scene spans the square [0, 100] x [0, 100]\n",
    "    add!(scene, Tree(Point(30, 20))) # place a tree at x=30, y=20\n",
    "    add!(scene, Tree(Point(83, 80)))\n",
    "    add!(scene, Tree(Point(80, 40)))\n",
    "    wall_height = 30.\n",
    "    add!(scene, Wall(Point(20., 40.), 1, 40., 2., wall_height))\n",
    "    add!(scene, Wall(Point(60., 40.), 2, 40., 2., wall_height))\n",
    "    add!(scene, Wall(Point(60.-15., 80.), 1, 15. + 2., 2., wall_height))\n",
    "    add!(scene, Wall(Point(20., 80.), 1, 15., 2., wall_height))\n",
    "    add!(scene, Wall(Point(20., 40.), 2, 40., 2., wall_height))    \n",
    "    \n",
    "    # time points at which we observe the agent's location\n",
    "    observation_times = collect(linspace(0.0, 200.0, 20)) ~ \"times\"\n",
    "    \n",
    "    # assumed speed of the agent\n",
    "    speed = 1.0\n",
    "    \n",
    "    # the starting location of the agent is a random point in the scene\n",
    "    start = Point(uniform(0, 100), uniform(0, 100)) ~ \"start\"\n",
    "    \n",
    "    # the destination of the agent is a random point in the scene\n",
    "    destination = Point(uniform(0, 100), uniform(0, 100)) ~ \"destination\"\n",
    "    \n",
    "    # the path of the agent from its start location to its destination\n",
    "    # uses a simple 2D holonomic path planner based on RRT (path_planner.jl)\n",
    "    (tree, rough_path, final_path) = plan_path(start, destination, scene)\n",
    "    \n",
    "    if isnull(final_path)\n",
    "        \n",
    "        # the agent could not find a path to its destination\n",
    "        # assume it stays at the start location indefinitely\n",
    "        locations = [start for _ in observation_times]\n",
    "    else\n",
    "        \n",
    "        # the agent found a path to its destination\n",
    "        # assume it moves from the start to the destinatoin along the path at constnat speed\n",
    "        # sample its location along this path for each time in observation times\n",
    "        locations = walk_path(get(final_path), speed, observation_times)\n",
    "    end\n",
    "    \n",
    "    # assume that the observed locations are noisy measurements of the true locations\n",
    "    # assume the noise is normally distributed with standard deviation 'noise'\n",
    "    noise = 1.0\n",
    "    for (i, t) in enumerate(observation_times)\n",
    "        measured_x = normal(locations[i].x, noise) ~ \"x$i\"\n",
    "        measured_y = normal(locations[i].y, noise) ~ \"y$i\"\n",
    "    end\n",
    "    \n",
    "    # record other program state for rendering\n",
    "    scene ~ \"scene\"\n",
    "end;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can run the model to generate probable scenarios:. We run a model using `@generate`, which executes a program and populates a trace with the values of the named expressions in the program.\n",
    "\n",
    "EXPLAIN THAT ANY PROGRAM STATE CAN BE TRACED INCLUDING RANDOM CHOICES AND NON-RANDOM CHOICES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start: Point(41.62377528866601,6.023750933344041)\n",
      "destination: Point(93.58995599592403,64.12110725135756)\n",
      "x1 through x4: [38.9629,47.3418,54.0598,64.667]\n",
      "y1 through y4: [5.05266,15.9794,21.2188,28.5684]\n"
     ]
    }
   ],
   "source": [
    "trace = Trace()\n",
    "@generate(trace, agent_model())\n",
    "println(\"start: \", value(trace, \"start\"))\n",
    "println(\"destination: \", value(trace, \"destination\"))\n",
    "println(\"x1 through x4: \", map((i) -> value(trace, \"x$i\"), 1:4))\n",
    "println(\"y1 through y4: \", map((i) -> value(trace, \"y$i\"), 1:4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we run it again, we get a different result:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start: Point(41.39195239154767,86.56500755895678)\n",
      "destination: Point(0.030669177779141243,46.42609827425972)\n",
      "x1 through x4: [40.8192,32.3991,20.6858,14.8003]\n",
      "y1 through y4: [86.5162,84.6837,81.4562,72.2509]\n"
     ]
    }
   ],
   "source": [
    "trace = Trace()\n",
    "@generate(trace, agent_model())\n",
    "println(\"start: \", value(trace, \"start\"))\n",
    "println(\"destination: \", value(trace, \"destination\"))\n",
    "println(\"x1 through x4: \", map((i) -> value(trace, \"x$i\"), 1:4))\n",
    "println(\"y1 through y4: \", map((i) -> value(trace, \"y$i\"), 1:4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can view the whole trace. For now, don't worry about constraints, interventions, or proposals. Note that the `recorded` section lists all the values that were recorde. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 297,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-- Constraints --\n",
      "x2 => 10\n",
      "y2 => 20\n",
      "x3 => 10\n",
      "y3 => 30\n",
      "-- Interventions --\n",
      "start => Point(10.0,10.0)\n",
      "destination => Point(90.0,90.0)\n",
      "-- Proposals --\n",
      "-- Recorded --\n",
      "times => [0.0,10.5263,21.0526,31.5789,42.1053,52.6316,63.1579,73.6842,84.2105,94.7368,105.263,115.789,126.316,136.842,147.368,157.895,168.421,178.947,189.474,200.0]\n",
      "x1 => 8.83997947888003\n",
      "y1 => 11.341483896885006\n",
      "x4 => 35.33840953247626\n",
      "y4 => 26.35067079865551\n",
      "x5 => 46.57803086936879\n",
      "y5 => 27.070667396525064\n",
      "x6 => 56.619593824570806\n",
      "y6 => 26.825819036207797\n",
      "x7 => 66.21799347978964\n",
      "y7 => 27.852266426999197\n",
      "x8 => 74.8121642921307\n",
      "y8 => 30.00755015282657\n",
      "x9 => 85.30058858539059\n",
      "y9 => 35.6151566249177\n",
      "x10 => 86.38245413317341\n",
      "y10 => 47.281996671419456\n",
      "x11 => 85.55774145581165\n",
      "y11 => 59.36349445613278\n",
      "x12 => 88.9165208250659\n",
      "y12 => 66.87120768182838\n",
      "x13 => 88.53546062544528\n",
      "y13 => 75.98136445763129\n",
      "x14 => 90.41550187974406\n",
      "y14 => 88.85363034834937\n",
      "x15 => 89.52756660132378\n",
      "y15 => 89.35750534627498\n",
      "x16 => 90.38749918925689\n",
      "y16 => 89.86928216713856\n",
      "x17 => 88.62222137978104\n",
      "y17 => 90.80526577853108\n",
      "x18 => 90.04959555175245\n",
      "y18 => 90.81409677981661\n",
      "x19 => 89.09658308663515\n",
      "y19 => 90.29881846354293\n",
      "x20 => 88.71010035380151\n",
      "y20 => 88.8761285102748\n",
      "scene => Scene(0.0,100.0,0.0,100.0,Obstacle[Tree(Point(30.0,20.0),10.0,Polygon(Point[Point(25.0,15.0),Point(35.0,15.0),Point(35.0,25.0),Point(25.0,25.0)]),:Tree),Tree(Point(83.0,80.0),10.0,Polygon(Point[Point(78.0,75.0),Point(88.0,75.0),Point(88.0,85.0),Point(78.0,85.0)]),:Tree),Tree(Point(80.0,40.0),10.0,Polygon(Point[Point(75.0,35.0),Point(85.0,35.0),Point(85.0,45.0),Point(75.0,45.0)]),:Tree),Wall(Point(20.0,40.0),1,40.0,2.0,30.0,Polygon(Point[Point(20.0,40.0),Point(60.0,40.0),Point(60.0,42.0),Point(20.0,42.0)]),:Wall),Wall(Point(60.0,40.0),2,40.0,2.0,30.0,Polygon(Point[Point(60.0,40.0),Point(62.0,40.0),Point(62.0,80.0),Point(60.0,80.0)]),:Wall),Wall(Point(45.0,80.0),1,17.0,2.0,30.0,Polygon(Point[Point(45.0,80.0),Point(62.0,80.0),Point(62.0,82.0),Point(45.0,82.0)]),:Wall),Wall(Point(20.0,80.0),1,15.0,2.0,30.0,Polygon(Point[Point(20.0,80.0),Point(35.0,80.0),Point(35.0,82.0),Point(20.0,82.0)]),:Wall),Wall(Point(20.0,40.0),2,40.0,2.0,30.0,Polygon(Point[Point(20.0,40.0),Point(22.0,40.0),Point(22.0,80.0),Point(20.0,80.0)]),:Wall)])\n"
     ]
    }
   ],
   "source": [
    "print(trace)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Visualizing the probabilistic behavior of a model using a trace rendering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Printing out the values of variables is not a very good way to understand the probabilistic behavior of a program. Instead, we use a **trace rendering** to produce a visual representation of the trace. The trace rendering encodes the trace into a representation that the human visual system can quickly interpret. In Gen.jl a trace renderer is simply object that has a method `render(trace::Trace)`. In Jupyter notebooks, we render traces using JavaScript code with a generic `JupyterInlineRenderer`. \n",
    "\n",
    "FIX ME\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 389,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "var Gen = require(\"nbextensions/gen_notebook_extension/main\");\n",
       "var d3 = require(\"nbextensions/d3/d3.min\");\n",
       "\n",
       "Gen.register_jupyter_renderer(\"agent_model_renderer\", function(id, trace) {\n",
       "    \n",
       "    var root = d3.select(\"#\" + id);\n",
       "    \n",
       "    // the base of the scene where all data is rendered\n",
       "    var svg = root.selectAll(\"svg\").data([\"\"]);\n",
       "    svg = svg.enter().append(\"svg\")\n",
       "        .attr(\"viewBox\", \"0 0 100 100\")\n",
       "        .attr(\"position\", \"absolute\")\n",
       "        .style(\"height\", \"100%\")\n",
       "        .merge(svg);\n",
       "    \n",
       "    // bounding box around the frame\n",
       "    svg.selectAll(\"rect\").data([\"\"]).enter().append(\"rect\")\n",
       "        .attr(\"width\", \"100%\")\n",
       "        .attr(\"height\", \"100%\")\n",
       "        .attr(\"stroke\", \"#000\")\n",
       "        .attr(\"fill\", \"#FFF\");\n",
       "    \n",
       "    // the path points of the agent\n",
       "    // TODO would be great if they could be observed together..\n",
       "    var times = Gen.find_choice(trace, \"times\").value;\n",
       "    var path_point_data = [];\n",
       "    for (var i=1; i<=times.length; i++) {\n",
       "        path_point_data.push({x: Gen.find_choice(trace, \"x\" + i),\n",
       "                              y: Gen.find_choice(trace, \"y\" + i)});\n",
       "    }\n",
       "    var path_segment_data = [];\n",
       "    for (var i=0; i<times.length-1; i++) {\n",
       "        path_segment_data.push({prev: {x: path_point_data[i].x,   y: path_point_data[i].y},\n",
       "                                next: {x: path_point_data[i+1].x, y: path_point_data[i+1].y}});\n",
       "    }\n",
       "    var radius = 2;\n",
       "    var path_points = svg.selectAll(\".path\").data(path_point_data);\n",
       "    path_points.exit().remove();\n",
       "    path_points.enter().append(\"circle\")\n",
       "        .attr(\"r\", radius)\n",
       "        .style(\"fill\", \"orange\")\n",
       "        .style(\"fill-opacity\", 0.5)\n",
       "        .classed(\"path\", true)\n",
       "      .merge(path_points)\n",
       "        .attr(\"cx\", function(d) { return d.x.value; })\n",
       "        .attr(\"cy\", function(d) { return d.y.value; });\n",
       "    svg.selectAll(\".path\")\n",
       "        .classed(\"interventions\", function(d) { return d.x.where == Gen.interventions || d.y.where == Gen.interventions; })\n",
       "        .classed(\"constraints\", function(d) { return d.x.where == Gen.constraints || d.y.where == Gen.constraints; });\n",
       "    \n",
       "    var path_segments = svg.selectAll(\".path_segments\").data(path_segment_data);\n",
       "    path_segments.exit().remove();\n",
       "    path_segments.enter().append(\"line\")\n",
       "        .style(\"stroke\", \"#000\")\n",
       "        .classed(\"path_segments\", true)\n",
       "      .merge(path_segments)\n",
       "        .attr(\"x1\", function(d) { return d.prev.x.value; })\n",
       "        .attr(\"y1\", function(d) { return d.prev.y.value; })\n",
       "        .attr(\"x2\", function(d) { return d.next.x.value; })\n",
       "        .attr(\"y2\", function(d) { return d.next.y.value; });\n",
       "    \n",
       "    // the starting position of the agent\n",
       "    var trace_start = Gen.find_choice(trace, \"start\");\n",
       "    var start = svg.selectAll(\".start\").data(trace_start ? [trace_start] : []);\n",
       "    start.exit().remove();\n",
       "    start.enter().append(\"circle\")\n",
       "        .attr(\"r\", radius)\n",
       "        .style(\"fill\", \"blue\")\n",
       "        .classed(\"start\", true)\n",
       "      .merge(start)\n",
       "        .attr(\"cx\", function(d) { return d.value.x; })\n",
       "        .attr(\"cy\", function(d) { return d.value.y; });\n",
       "    \n",
       "    // the destination position of the agent\n",
       "    var trace_dest = Gen.find_choice(trace, \"destination\");\n",
       "    var dest = svg.selectAll(\".destination\").data(trace_dest ? [trace_dest] : []);\n",
       "    dest.exit().remove();\n",
       "    dest.enter().append(\"circle\")\n",
       "        .attr(\"r\", radius)\n",
       "        .style(\"fill\", \"red\")\n",
       "        .classed(\"destination\", true)\n",
       "      .merge(dest)\n",
       "        .attr(\"cx\", function(d) { return d.value.x; })\n",
       "        .attr(\"cy\", function(d) { return d.value.y; });\n",
       "\n",
       "    // whether the start and destination are intervened or constrained\n",
       "    svg.selectAll(\".destinations, .start\")\n",
       "        .classed(\"interventions\", function(d) { return d.where == Gen.interventions; })\n",
       "        .classed(\"constraints\", function(d) { return d.where == Gen.constraints; });\n",
       "\n",
       "    // apply styles to indicate intervened or constrained\n",
       "    svg.selectAll(\".interventions\")\n",
       "        .style(\"stroke\", \"#000\")\n",
       "        .style(\"stroke-width\", 2);\n",
       "    svg.selectAll(\".constraints\")\n",
       "        .style(\"stroke\", \"#000\")\n",
       "        .style(\"stroke-width\", 2)\n",
       "        .style(\"stroke-dasharray\", \"1, 1\");\n",
       "    \n",
       "    // make the legend\n",
       "    // TODO\n",
       "    \n",
       "    \n",
       "    // Draw the obstacles in the scene\n",
       "    \n",
       "    var trace_scene = Gen.find_choice(trace, \"scene\");\n",
       "    \n",
       "    var trace_trees = trace_scene.value.obstacles.filter(function(element) { return element.name == \"Tree\";});\n",
       "    var trees = svg.selectAll(\".tree\").data(trace_trees);\n",
       "    trees.exit().remove();\n",
       "    trees.enter().append(\"rect\")\n",
       "        .style(\"fill\", \"green\")\n",
       "        .classed(\"tree\", true)\n",
       "      .merge(trees)\n",
       "        .attr(\"x\", function(d) { return d.center.x - d.size/2.0; })\n",
       "        .attr(\"y\", function(d) { return d.center.y - d.size/2.0; })\n",
       "        .attr(\"width\", function(d) { return d.size; })\n",
       "        .attr(\"height\", function(d) { return d.size; });\n",
       "    \n",
       "    var trace_walls = trace_scene.value.obstacles.filter(function(element) { return element.name == \"Wall\";});\n",
       "    var walls = svg.selectAll(\".wall\").data(trace_walls);\n",
       "    walls.exit().remove();\n",
       "    walls.enter().append(\"rect\")\n",
       "        .style(\"fill\", \"gray\")\n",
       "        .classed(\"wall\", true)\n",
       "      .merge(walls)\n",
       "        .attr(\"x\", function(d) { return d.start.x; })\n",
       "        .attr(\"y\", function(d) { return d.start.y; })\n",
       "        .attr(\"width\", function(d) { return d.orientation == 1 ? d.length : d.thickness; })\n",
       "        .attr(\"height\", function(d) { return d.orientation == 2 ? d.length : d.thickness; });\n",
       "    \n",
       "    // TODO add the log score\n",
       "    var score = svg.selectAll(\".score\").data([\"\"]);\n",
       "    score.enter().append(\"text\")\n",
       "        .classed(\"score\", true)\n",
       "        .attr(\"x\", 50).attr(\"y\", 95).attr(\"text-anchor\", \"middle\")\n",
       "        .attr(\"font-size\", \"10px\")\n",
       "      .merge(score)\n",
       "        .text(trace.log_weight.toFixed(2))\n",
       "    \n",
       "});\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "javascript\"\"\"\n",
    "\n",
    "var Gen = require(\"nbextensions/gen_notebook_extension/main\");\n",
    "var d3 = require(\"nbextensions/d3/d3.min\");\n",
    "\n",
    "Gen.register_jupyter_renderer(\"agent_model_renderer\", function(id, trace) {\n",
    "    \n",
    "    var root = d3.select(\"#\" + id);\n",
    "    \n",
    "    // the base of the scene where all data is rendered\n",
    "    var svg = root.selectAll(\"svg\").data([\"\"]);\n",
    "    svg = svg.enter().append(\"svg\")\n",
    "        .attr(\"viewBox\", \"0 0 100 100\")\n",
    "        .attr(\"position\", \"absolute\")\n",
    "        .style(\"height\", \"100%\")\n",
    "        .merge(svg);\n",
    "    \n",
    "    // bounding box around the frame\n",
    "    svg.selectAll(\"rect\").data([\"\"]).enter().append(\"rect\")\n",
    "        .attr(\"width\", \"100%\")\n",
    "        .attr(\"height\", \"100%\")\n",
    "        .attr(\"stroke\", \"#000\")\n",
    "        .attr(\"fill\", \"#FFF\");\n",
    "    \n",
    "    // the path points of the agent\n",
    "    // TODO would be great if they could be observed together..\n",
    "    var times = Gen.find_choice(trace, \"times\").value;\n",
    "    var path_point_data = [];\n",
    "    for (var i=1; i<=times.length; i++) {\n",
    "        path_point_data.push({x: Gen.find_choice(trace, \"x\" + i),\n",
    "                              y: Gen.find_choice(trace, \"y\" + i)});\n",
    "    }\n",
    "    var path_segment_data = [];\n",
    "    for (var i=0; i<times.length-1; i++) {\n",
    "        path_segment_data.push({prev: {x: path_point_data[i].x,   y: path_point_data[i].y},\n",
    "                                next: {x: path_point_data[i+1].x, y: path_point_data[i+1].y}});\n",
    "    }\n",
    "    var radius = 2;\n",
    "    var path_points = svg.selectAll(\".path\").data(path_point_data);\n",
    "    path_points.exit().remove();\n",
    "    path_points.enter().append(\"circle\")\n",
    "        .attr(\"r\", radius)\n",
    "        .style(\"fill\", \"orange\")\n",
    "        .style(\"fill-opacity\", 0.5)\n",
    "        .classed(\"path\", true)\n",
    "      .merge(path_points)\n",
    "        .attr(\"cx\", function(d) { return d.x.value; })\n",
    "        .attr(\"cy\", function(d) { return d.y.value; });\n",
    "    svg.selectAll(\".path\")\n",
    "        .classed(\"interventions\", function(d) { return d.x.where == Gen.interventions || d.y.where == Gen.interventions; })\n",
    "        .classed(\"constraints\", function(d) { return d.x.where == Gen.constraints || d.y.where == Gen.constraints; });\n",
    "    \n",
    "    var path_segments = svg.selectAll(\".path_segments\").data(path_segment_data);\n",
    "    path_segments.exit().remove();\n",
    "    path_segments.enter().append(\"line\")\n",
    "        .style(\"stroke\", \"#000\")\n",
    "        .classed(\"path_segments\", true)\n",
    "      .merge(path_segments)\n",
    "        .attr(\"x1\", function(d) { return d.prev.x.value; })\n",
    "        .attr(\"y1\", function(d) { return d.prev.y.value; })\n",
    "        .attr(\"x2\", function(d) { return d.next.x.value; })\n",
    "        .attr(\"y2\", function(d) { return d.next.y.value; });\n",
    "    \n",
    "    // the starting position of the agent\n",
    "    var trace_start = Gen.find_choice(trace, \"start\");\n",
    "    var start = svg.selectAll(\".start\").data(trace_start ? [trace_start] : []);\n",
    "    start.exit().remove();\n",
    "    start.enter().append(\"circle\")\n",
    "        .attr(\"r\", radius)\n",
    "        .style(\"fill\", \"blue\")\n",
    "        .classed(\"start\", true)\n",
    "      .merge(start)\n",
    "        .attr(\"cx\", function(d) { return d.value.x; })\n",
    "        .attr(\"cy\", function(d) { return d.value.y; });\n",
    "    \n",
    "    // the destination position of the agent\n",
    "    var trace_dest = Gen.find_choice(trace, \"destination\");\n",
    "    var dest = svg.selectAll(\".destination\").data(trace_dest ? [trace_dest] : []);\n",
    "    dest.exit().remove();\n",
    "    dest.enter().append(\"circle\")\n",
    "        .attr(\"r\", radius)\n",
    "        .style(\"fill\", \"red\")\n",
    "        .classed(\"destination\", true)\n",
    "      .merge(dest)\n",
    "        .attr(\"cx\", function(d) { return d.value.x; })\n",
    "        .attr(\"cy\", function(d) { return d.value.y; });\n",
    "\n",
    "    // whether the start and destination are intervened or constrained\n",
    "    svg.selectAll(\".destinations, .start\")\n",
    "        .classed(\"interventions\", function(d) { return d.where == Gen.interventions; })\n",
    "        .classed(\"constraints\", function(d) { return d.where == Gen.constraints; });\n",
    "\n",
    "    // apply styles to indicate intervened or constrained\n",
    "    svg.selectAll(\".interventions\")\n",
    "        .style(\"stroke\", \"#000\")\n",
    "        .style(\"stroke-width\", 2);\n",
    "    svg.selectAll(\".constraints\")\n",
    "        .style(\"stroke\", \"#000\")\n",
    "        .style(\"stroke-width\", 2)\n",
    "        .style(\"stroke-dasharray\", \"1, 1\");\n",
    "    \n",
    "    // make the legend\n",
    "    // TODO\n",
    "    \n",
    "    \n",
    "    // Draw the obstacles in the scene\n",
    "    \n",
    "    var trace_scene = Gen.find_choice(trace, \"scene\");\n",
    "    \n",
    "    var trace_trees = trace_scene.value.obstacles.filter(function(element) { return element.name == \"Tree\";});\n",
    "    var trees = svg.selectAll(\".tree\").data(trace_trees);\n",
    "    trees.exit().remove();\n",
    "    trees.enter().append(\"rect\")\n",
    "        .style(\"fill\", \"green\")\n",
    "        .classed(\"tree\", true)\n",
    "      .merge(trees)\n",
    "        .attr(\"x\", function(d) { return d.center.x - d.size/2.0; })\n",
    "        .attr(\"y\", function(d) { return d.center.y - d.size/2.0; })\n",
    "        .attr(\"width\", function(d) { return d.size; })\n",
    "        .attr(\"height\", function(d) { return d.size; });\n",
    "    \n",
    "    var trace_walls = trace_scene.value.obstacles.filter(function(element) { return element.name == \"Wall\";});\n",
    "    var walls = svg.selectAll(\".wall\").data(trace_walls);\n",
    "    walls.exit().remove();\n",
    "    walls.enter().append(\"rect\")\n",
    "        .style(\"fill\", \"gray\")\n",
    "        .classed(\"wall\", true)\n",
    "      .merge(walls)\n",
    "        .attr(\"x\", function(d) { return d.start.x; })\n",
    "        .attr(\"y\", function(d) { return d.start.y; })\n",
    "        .attr(\"width\", function(d) { return d.orientation == 1 ? d.length : d.thickness; })\n",
    "        .attr(\"height\", function(d) { return d.orientation == 2 ? d.length : d.thickness; });\n",
    "    \n",
    "    // TODO add the log score\n",
    "    var score = svg.selectAll(\".score\").data([\"\"]);\n",
    "    score.enter().append(\"text\")\n",
    "        .classed(\"score\", true)\n",
    "        .attr(\"x\", 50).attr(\"y\", 95).attr(\"text-anchor\", \"middle\")\n",
    "        .attr(\"font-size\", \"10px\")\n",
    "      .merge(score)\n",
    "        .text(trace.log_weight.toFixed(2))\n",
    "    \n",
    "});\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 390,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div style=\"border-style: solid; border-radius: 25px; text-align: center;\">\n",
       "Trace Rendering (agent_model_renderer)\n",
       "<div id=\"id_MVIoFDb6mj92O5cTJgOt\" style=\"height: 200px; resize: vertical; position: relative; overflow: hidden; text-align: center;\">\n",
       "</div>\n",
       "</div>\n",
       "    "
      ],
      "text/plain": [
       "HTML{String}(\"<div style=\\\"border-style: solid; border-radius: 25px; text-align: center;\\\">\\nTrace Rendering (agent_model_renderer)\\n<div id=\\\"id_MVIoFDb6mj92O5cTJgOt\\\" style=\\\"height: 200px; resize: vertical; position: relative; overflow: hidden; text-align: center;\\\">\\n</div>\\n</div>\\n    \")"
      ]
     },
     "execution_count": 390,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "renderer = JupyterInlineRenderer(\"agent_model_renderer\")\n",
    "viewport(renderer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's use this rendering to visualize a run of the our program. It will be visualized in the rendering view above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 391,
   "metadata": {},
   "outputs": [],
   "source": [
    "trace = Trace()\n",
    "intervene!(trace, \"start\", Point(10, 10))\n",
    "intervene!(trace, \"destination\", Point(90, 90))\n",
    "constrain!(trace, \"x2\", 10)\n",
    "constrain!(trace, \"y2\", 20)\n",
    "constrain!(trace, \"x3\", 10)\n",
    "constrain!(trace, \"y3\", 30)\n",
    "for i=1:1\n",
    "    @generate(trace, agent_model())\n",
    "    render(renderer, trace)\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the behavior is stochastic, we need to visualize many samples at once in a grid, to get a sense of the full distribution:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "LoadError",
     "evalue": "UndefVarError: inline not defined",
     "output_type": "error",
     "traceback": [
      "UndefVarError: inline not defined",
      ""
     ]
    }
   ],
   "source": [
    "tiled = TiledJupyterInlineRenderer(\"agent_model_renderer\", 3, 1, 900, 300)\n",
    "inline(tiled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "LoadError",
     "evalue": "No target has been defined",
     "output_type": "error",
     "traceback": [
      "No target has been defined",
      "",
      " in render(::Gen.TiledJupyterInlineRenderer, ::Array{Gen.Trace,1}) at /home/marcoct/dev/Gen.jl/src/notebook.jl:86"
     ]
    }
   ],
   "source": [
    "traces = [begin trace = Trace(); @generate(trace, agent_model()); trace end for i=1:3]\n",
    "render(tiled, traces)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can intervene and see what the traces look like when we fix the value of `start` and `destination` to specific values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "LoadError",
     "evalue": "UndefVarError: inline not defined",
     "output_type": "error",
     "traceback": [
      "UndefVarError: inline not defined",
      ""
     ]
    }
   ],
   "source": [
    "tiled = TiledJupyterInlineRenderer(\"agent_model_renderer\", 9, 3, 900, 300)\n",
    "inline(tiled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "ename": "LoadError",
     "evalue": "No target has been defined",
     "output_type": "error",
     "traceback": [
      "No target has been defined",
      "",
      " in render(::Gen.TiledJupyterInlineRenderer, ::Array{Gen.Trace,1}) at /home/marcoct/dev/Gen.jl/src/notebook.jl:86"
     ]
    }
   ],
   "source": [
    "trace = Trace()\n",
    "intervene!(trace, \"start\", Point(10, 10))\n",
    "intervene!(trace, \"destination\", Point(90, 90))\n",
    "traces = Trace[]\n",
    "for i=1:27\n",
    "    t = deepcopy(trace)\n",
    "    @generate(t, agent_model())\n",
    "    push!(traces, t)\n",
    "end\n",
    "render(tiled, traces)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Probabilistic inference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So far, we have simulated forward from the model, and we have intervened on some random choices and simulated the consequence. However, suppose we had observed a given sequence of locations of the agent, and we wanted to know probable goal locations? This is a query that cannot be answered simply by forward simulation of the program, because the location of the drone is a *consequence* and not a *cause* of the destination. We can easily find probable consequences given the causes, but finding probable causes given the consequences requires a bit more work."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is an example dataset showing measured locations for the first 15 time points:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4-element Array{Point,1}:\n",
       " Point(10.0,10.0)\n",
       " Point(10.0,20.0)\n",
       " Point(10.0,30.0)\n",
       " Point(10.0,40.0)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "points = [Point(10, 10), Point(10, 20), Point(10, 30), Point(10, 40)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first step in inference is to constrain the random choices that are observed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "trace = Trace()\n",
    "intervene!(trace, \"start\", Point(10, 10))\n",
    "for (i, point) in enumerate(points)\n",
    "    constrain!(trace, \"x$i\", point.x)\n",
    "    constrain!(trace, \"y$i\", point.y)\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, when we run the program in this trace, we find that the score of the trace tells us how well the trace matches the constraints."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "ename": "LoadError",
     "evalue": "UndefVarError: inline not defined",
     "output_type": "error",
     "traceback": [
      "UndefVarError: inline not defined",
      ""
     ]
    }
   ],
   "source": [
    "tiled = TiledJupyterInlineRenderer(\"agent_model_renderer\", 9, 3, 900, 300)\n",
    "inline(tiled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "ename": "LoadError",
     "evalue": "No target has been defined",
     "output_type": "error",
     "traceback": [
      "No target has been defined",
      "",
      " in render(::Gen.TiledJupyterInlineRenderer, ::Array{Gen.Trace,1}) at /home/marcoct/dev/Gen.jl/src/notebook.jl:86"
     ]
    }
   ],
   "source": [
    "traces = Trace[]\n",
    "for i=1:27\n",
    "    t = deepcopy(trace)\n",
    "    @generate(t, agent_model())\n",
    "    push!(traces, t)\n",
    "end\n",
    "render(tiled, traces)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importance sampling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use this score to filter out traces that don't match well with the observations. Specifically, we can sample a large number of traces, and pick one in proportion to the exponential of its score. We can then repreat this whole process a number of times:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "LoadError",
     "evalue": "UndefVarError: inline not defined",
     "output_type": "error",
     "traceback": [
      "UndefVarError: inline not defined",
      ""
     ]
    }
   ],
   "source": [
    "# TODO create an HTML helper that creates an array of SVG elements, that can then be filled in one by one\n",
    "# while the algorithm is running.\n",
    "importance_sampling_result_renderer = TiledJupyterInlineRenderer(\"agent_model_renderer\", 9, 3, 900, 300)\n",
    "inline(importance_sampling_result_renderer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "ename": "LoadError",
     "evalue": "No target has been defined",
     "output_type": "error",
     "traceback": [
      "No target has been defined",
      "",
      " in render(::Gen.TiledJupyterInlineRenderer, ::Array{Gen.Trace,1}) at /home/marcoct/dev/Gen.jl/src/notebook.jl:86"
     ]
    }
   ],
   "source": [
    "function logsumexp(arr::Vector{Float64})\n",
    "    min_arr = maximum(arr)\n",
    "    min_arr + log(sum(exp(arr - min_arr)))\n",
    "end\n",
    "\n",
    "approximate_samples = Trace[]\n",
    "num_approximate_samples = 27\n",
    "num_simulations = 10\n",
    "for i=1:num_approximate_samples\n",
    "    traces = Vector{Trace}(num_simulations)\n",
    "    scores = Vector{Float64}(num_simulations)\n",
    "    for i=1:num_simulations\n",
    "        t = deepcopy(trace)\n",
    "        @generate(t, agent_model())\n",
    "        scores[i] = score(t)\n",
    "        traces[i] = t\n",
    "    end\n",
    "    weights = exp(scores - logsumexp(scores))\n",
    "    weights = weights / sum(weights)\n",
    "    chosen = rand(Categorical(weights))\n",
    "    push!(approximate_samples, traces[chosen])\n",
    "end\n",
    "# TODO render incrementally!\n",
    "render(importance_sampling_result_renderer, approximate_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# show compositing of the goal rendering...\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# show SIR results in a tile plots"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Metropolis-Hastings Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "        <svg id=\"current\" height=\"200\" viewBox=\"0 0 100 100\"></svg>\n",
       "        <svg id=\"proposed\" height=\"200\" viewBox=\"0 0 100 100\"></svg>\n"
      ],
      "text/plain": [
       "HTML{String}(\"        <svg id=\\\"current\\\" height=\\\"200\\\" viewBox=\\\"0 0 100 100\\\"></svg>\\n        <svg id=\\\"proposed\\\" height=\\\"200\\\" viewBox=\\\"0 0 100 100\\\"></svg>\\n\")"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "current_trace_renderer = JupyterInlineRenderer(\"agent_model_renderer\")\n",
    "attach(current_trace_renderer, \"current\");\n",
    "proposed_trace_renderer = JupyterInlineRenderer(\"agent_model_renderer\")\n",
    "attach(proposed_trace_renderer, \"proposed\");\n",
    "HTML(\"\"\"\n",
    "        <svg id=\"current\" height=\"200\" viewBox=\"0 0 100 100\"></svg>\n",
    "        <svg id=\"proposed\" height=\"200\" viewBox=\"0 0 100 100\"></svg>\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "ename": "LoadError",
     "evalue": "MethodError: no method matching agent_model(::Gen.Trace, ::#agent_model)\u001b[0m\nClosest candidates are:\n  agent_model(::Gen.AbstractTrace) at In[3]:4\u001b[0m",
     "output_type": "error",
     "traceback": [
      "MethodError: no method matching agent_model(::Gen.Trace, ::#agent_model)\u001b[0m\nClosest candidates are:\n  agent_model(::Gen.AbstractTrace) at In[3]:4\u001b[0m",
      "",
      " in macro expansion; at /home/marcoct/dev/Gen.jl/src/trace.jl:325 [inlined]",
      " in macro expansion; at ./In[22]:11 [inlined]",
      " in anonymous at ./<missing>:?"
     ]
    }
   ],
   "source": [
    "current_trace = Trace()\n",
    "intervene!(current_trace, \"start\", Point(10, 10))\n",
    "for (i, point) in enumerate(points)\n",
    "    constrain!(current_trace, \"x$i\", point.x)\n",
    "    constrain!(current_trace, \"y$i\", point.y)\n",
    "end\n",
    "@generate(current_trace, agent_model())\n",
    "current_score = score(current_trace)\n",
    "for i=1:1000\n",
    "    proposed_trace = deepcopy(current_trace)\n",
    "    @generate(proposed_trace, agent_model(agent_model))\n",
    "    proposed_score = score(proposed_trace)\n",
    "    if log(rand()) < proposed_score - current_score\n",
    "        current_trace = proposed_trace\n",
    "        current_score = proposed_score\n",
    "    end\n",
    "    render(current_trace_renderer, current_trace)\n",
    "    render(proposed_trace_renderer, proposed_trace)\n",
    "    sleep(0.1)\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Improving the model\n",
    "\n",
    "Our model above made a lot of assumptions that are unlikely to hold in the real world. For example, the agent always takes pretty direct paths from its starting location to its final destination. What if the agent is more unpredictable? What if it takes detours?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is a dataset that does not match our model's expectations. Let's see what happens when we try to do probabilistic inference in our model, given this data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# show results (they should look bad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is an example of *model mis-specification*, which is when the model is not a good match for the distribution of data we are analyzing.\n",
    "\n",
    "We can improve the model by adding the possibliltiy thathte agent takes a detour. Specifically, we add the possibility that the agent uses a waypoint and first walks from the starting locatoin to the waypoint and then from the waypoit to the final destination. Let's visualize the resulting samples:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# render_grid()..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we can run inference as before:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Do SIR with the new dataset and the improved model, and the same number of particles we used above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# results for the same number of particles as above should look bad."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It looks like the results are not accurate. This is because in the new model, a random forward execution of the imporved model is a lot less likely to match the observations than a random forward exection of the original program. We can try to increase the number of samples to incrase the probability that we get one that matches the data. Note that this will take a few minutes t orun:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Show results with a larger number of particles (should take < 1 min, should look okay)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This modification of the model mad einference a lot more computationally challenging, and our importance sampling algorihtm is not able to give us real-time inferneces. This motivates the need for a more sophisicated approach to probabilisitc inference."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Compiling inference with neural networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are a number of approaches for creating more efficient inference algorithms. We will focus on one approach, where we train a neural network to make informed guesses about he locatio nof the waypoint. First, let's understand in a bit more detail why the default inference algorihtm was slow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Suppose we knew the right waypoint:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Point(0.5,0.5)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trace = Trace()\n",
    "constrain!(trace, \"use-waypoint\", true)\n",
    "constrain!(trace, \"waypoint\", Point(0.5, 0.5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then the baseline importance sapmling algorithm gives reasonable inferences with fewer samples. We use this idea by training a neural network to make informed guesses about the waypoint, given the observed data as its input. We train the neural network on nany simulatoins of the program. Then, the resulting trained neural network can be used to make informed guesses about the waypooint given any observations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# show the neural network, and show the training."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's visualize the guesses made by the neural network for a few  different datasets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# show four renderings left to right of different datasets, with circles denoting the neural network's guess about the waypoint. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's use this trained neural network to speed up inference:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# show the modififed SIR algorithm, using propose!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# show results for fewer particles, which should be noticeably faster than without the neural network.\n",
    "# make an explicit comparison."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 0.5.1",
   "language": "julia",
   "name": "julia-0.5"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "0.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
